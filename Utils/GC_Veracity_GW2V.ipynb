{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Veracity_GW2V.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "dqy1yG6tYWTf",
        "colab_type": "code",
        "outputId": "081ae177-26ef-4827-e4a8-7c5cf3a22338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ft0aAvOphOAd",
        "colab_type": "code",
        "outputId": "08e8ab51-b7a8-4572-bdcd-b2e4c32855fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Rumour/Try\")\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Rumour/Try'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "NAs9rJiBiCXu",
        "colab_type": "code",
        "outputId": "bf2bc2da-803a-4366-e385-2ca3c61c8a2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!wget \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-09 13:48:45--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.100.117\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.100.117|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz.2’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  55.6MB/s    in 29s     \n",
            "\n",
            "2019-02-09 13:49:13 (54.8 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz.2’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "64VIm6S_iBzd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "with gzip.open('GoogleNews-vectors-negative300.bin.gz','rb') as f:\n",
        "  file_content=f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hTmS_SfpjCfY",
        "colab_type": "code",
        "outputId": "65ace369-56b8-4a0b-d898-07e6f5e66c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "open('w2v.bin','wb').write(file_content)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3644258522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "_b0JGhLTn5z1",
        "colab_type": "code",
        "outputId": "7da71c35-72f9-4d6d-8d20-32bc2e2b1c38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "e-KykybMxhh2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Function in this file extracts features from conversations(threads) and returns\n",
        "dictionary with features\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "import re,pickle\n",
        "import help_prep_functions\n",
        "from textblob import TextBlob\n",
        "#from tree2branches import tree2branches\n",
        "from Lexicons import false_antonyms,false_synonyms,negation_words,SpeechAct\n",
        "with open('vocab2vec.pkl','rb') as r:\n",
        "  d=pickle.load(r)\n",
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "wh_words = ['what', 'when', 'where', 'which', 'who', 'whom', 'whose', 'why','how']\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "\n",
        "def content_formatting(text,content):\n",
        "\tif text.find(content)>=0:\n",
        "\t\treturn 1\n",
        "\telse:\n",
        "\t\treturn 0\n",
        "\n",
        "def count_lexicon(tokens,lexicon_list):\n",
        "\tcount=0\n",
        "\tfor token in tokens:\n",
        "\t\tif token in lexicon_list:\n",
        "\t\t\tcount += 1\n",
        "\treturn count\n",
        "\n",
        "def get_speechAct_vector(text):\n",
        "\tsv=[]\n",
        "\tfor key in SpeechAct.keys():\n",
        "\t\tcount=0\n",
        "\t\tfor verb in SpeechAct[key]:\n",
        "\t\t\tif verb in text.lower():\n",
        "\t\t\t\tcount+=1\n",
        "\t\tsv.append(count)\n",
        "\treturn sv\n",
        "\t\n",
        "def extract_source_features(conversation):\n",
        "    features=[]\n",
        "\n",
        "    tw = conversation['source']\n",
        "    tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+', '',tw['text'].lower()))\n",
        "\n",
        "    otherthreadtweets = ''\n",
        "    for response in conversation['replies']:\n",
        "        otherthreadtweets += ' ' + response['text']\n",
        "            \n",
        "    otherthreadtokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+', '',otherthreadtweets.lower()))\n",
        "\n",
        "    source_tweet = tw['text']\n",
        "    features.extend(help_prep_functions.sumw2v(tw, avg=True))\n",
        "    features.append(content_formatting(source_tweet,'?'))\n",
        "    features.append(content_formatting(source_tweet,'!'))\n",
        "    features.append(content_formatting(source_tweet,'.'))\n",
        "    features.append(content_formatting(source_tweet,'#'))\n",
        "    hasurl=0\n",
        "    if (content_formatting(source_tweet,'urlurlurl')>=0) or (content_formatting(source_tweet,'http') >= 0):\n",
        "        hasurl = 1\n",
        "    features.append(hasurl)\n",
        "    haspic=0\n",
        "    if (content_formatting(source_tweet,'picpicpic') >= 0) or (content_formatting(source_tweet,'pic.twitter.com') >= 0) or (content_formatting(source_tweet,'instagr.am') >= 0):\n",
        "        haspic=1\n",
        "    features.append(haspic)\n",
        "    negation_count= 0\n",
        "    for negationword in negation_words:\n",
        "        if negationword in tokens:\n",
        "            negation_count += 1\n",
        "    features.append(negation_count)\n",
        "    features.append(len(source_tweet))\t#Chararcter count\n",
        "    features.append(len(nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','', source_tweet.lower()))))\t#Word count\n",
        "    swearwords = []\n",
        "    with open('data/badwords.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            swearwords.append(line.strip().lower())\n",
        "    hasswearwords = 0\n",
        "    for token in tokens:\n",
        "        if token in swearwords:\n",
        "            hasswearwords += 1\n",
        "    features.append(hasswearwords)\n",
        "    uppers = [l for l in source_tweet if l.isupper()]\n",
        "    features.append(float(len(uppers))/len(source_tweet))\t#Capital Ratio\n",
        "    #feature_dict['avgw2v'] = help_prep_functions.sumw2v(tw, avg=True)\n",
        "    features.append(check_pos_tag(source_tweet, 'noun'))\n",
        "    features.append(check_pos_tag(source_tweet, 'verb'))\n",
        "    features.append(check_pos_tag(source_tweet, 'adj'))\n",
        "    features.append(check_pos_tag(source_tweet, 'adv'))\n",
        "    features.append(check_pos_tag(source_tweet, 'pron'))\n",
        "    features.append(count_lexicon(tokens,false_synonyms)) #src\n",
        "    features.append(count_lexicon(tokens,false_antonyms))#src\n",
        "    features.append(count_lexicon(otherthreadtokens,false_synonyms)) #Thread\n",
        "    features.append(count_lexicon(otherthreadtokens,false_antonyms)) #Thread\n",
        "    features.append(count_lexicon(tokens,wh_words))\n",
        "    features.append(count_lexicon(otherthreadtokens,wh_words)) #Thread\n",
        "    \n",
        "    #features.extend(get_speechAct_vector(source_tweet))\n",
        "    #features.extend(get_speechAct_vector(otherthreadtweets))\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def extract_thread_features_incl_response(conversation):\n",
        "    source_features = extract_source_features(conversation)\n",
        "    source_features.append(1)\t#Source_tweets Flag\n",
        "    srctokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+', '',conversation['source']['text'].lower()))\n",
        "    fullthread_featdict = {}\n",
        "    src=np.asarray(source_features)\n",
        "    fullthread_featdict[conversation['source']['id_str']] = src\n",
        "    for tw in conversation['replies']:\n",
        "        features=[]\n",
        "        tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+', '',tw['text'].lower()))\n",
        "        otherthreadtweets = ''\n",
        "        otherthreadtweets += conversation['source']['text'] \n",
        "   \n",
        "            \n",
        "        for response in conversation['replies']:\n",
        "            otherthreadtweets += ' ' + response['text']\n",
        "                \n",
        "        otherthreadtokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+', '',otherthreadtweets.lower()))\n",
        "        branches = conversation['branches']\n",
        "        for branch in branches:\n",
        "            if tw['id_str'] in branch:\n",
        "                if branch.index(tw['id_str'])-1 == 0:\n",
        "                    prevtokens = srctokens\n",
        "                else:\n",
        "                    prev_id = branch[branch.index(tw['id_str'])-1]\n",
        "                    for ptw in conversation['replies']:\n",
        "                        if ptw['id_str'] == prev_id:\n",
        "                            prevtokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','',ptw['text'].lower()))\n",
        "                            break\n",
        "            else:\n",
        "                prevtokens = []\n",
        "            break\n",
        "        reply_tweet = tw['text']\n",
        "        features.extend(help_prep_functions.sumw2v(tw, avg=True))\n",
        "        features.append(content_formatting(reply_tweet,'?'))\n",
        "        features.append(content_formatting(reply_tweet,'!'))\n",
        "        features.append(content_formatting(reply_tweet,'.'))\n",
        "        features.append(content_formatting(reply_tweet,'#'))\n",
        "        hasurl=0\n",
        "        if (content_formatting(reply_tweet,'urlurlurl')>=0) or (content_formatting(reply_tweet,'http') >= 0):\n",
        "            hasurl = 1\n",
        "        features.append(hasurl)\n",
        "        haspic=0\n",
        "        if (content_formatting(reply_tweet,'picpicpic') >= 0) or (content_formatting(reply_tweet,'pic.twitter.com') >= 0) or (content_formatting(reply_tweet,'instagr.am') >= 0):\n",
        "            haspic = 1\n",
        "        features.append(haspic)\n",
        "        negation_count= 0\n",
        "        for negationword in negation_words:\n",
        "            if negationword in tokens:\n",
        "                negation_count += 1\n",
        "        features.append(negation_count)\n",
        "        features.append(len(reply_tweet))\t#Chararcter count\n",
        "        features.append(len(nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','', reply_tweet.lower()))))\t#Word count\n",
        "        swearwords = []\n",
        "        with open('data/badwords.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                swearwords.append(line.strip().lower())\n",
        "                hasswearwords = 0\n",
        "                for token in tokens:\n",
        "                    if token in swearwords:\n",
        "                        hasswearwords += 1\n",
        "        features.append(hasswearwords)\n",
        "        uppers = [l for l in reply_tweet if l.isupper()]\n",
        "        if len(reply_tweet)==0:\n",
        "          features.append(0)\n",
        "          print (\"LOL\")\n",
        "        else:\n",
        "          features.append(float(len(uppers))/len(reply_tweet))\t#Capital Ratio\n",
        "        #feature_dict['avgw2v'] = help_prep_functions.sumw2v(tw, avg=True)\n",
        "        features.append(check_pos_tag(reply_tweet, 'noun'))\n",
        "        features.append(check_pos_tag(reply_tweet, 'verb'))\n",
        "        features.append(check_pos_tag(reply_tweet, 'adj'))\n",
        "        features.append(check_pos_tag(reply_tweet, 'adv'))\n",
        "        features.append(check_pos_tag(reply_tweet, 'pron'))\n",
        "        features.append(count_lexicon(tokens,false_synonyms)) #rep\n",
        "        features.append(count_lexicon(tokens,false_antonyms))#rep\n",
        "        features.append(count_lexicon(otherthreadtokens,false_synonyms)) #Thread\n",
        "        features.append(count_lexicon(otherthreadtokens,false_antonyms)) #Thread\n",
        "        features.append(count_lexicon(tokens,wh_words))\n",
        "        features.append(count_lexicon(otherthreadtokens,wh_words)) #Thread\n",
        "        #features.extend(get_speechAct_vector(reply_tweet))\n",
        "        #features.extend(get_speechAct_vector(otherthreadtweets))\t#Thread\n",
        "        features.append(0)\n",
        "        \n",
        "        #feature_dict['avgw2v'] = help_prep_functions.sumw2v(tw, avg=True)\n",
        "        rep=np.asarray(features)\n",
        "\t\t\n",
        "        fullthread_featdict[tw['id_str']] = rep\t\n",
        "    return fullthread_featdict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E8KHcbX8hyQU",
        "colab_type": "code",
        "outputId": "dc67cc1b-bba9-4532-8a6c-c516be74861e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#from extract_thread_features_W2V import extract_thread_features_incl_response\n",
        "import help_prep_functions\n",
        "import numpy as np\n",
        "import os,json\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "label_veracity={\"true\":0,\"false\":1,\"unverified\":2}\n",
        "\n",
        "def convert_to_array_of_branches(thread_feature_dict, conversation):\n",
        "    thread_features_array = []\n",
        "\n",
        "    branches = conversation['branches']\n",
        "\n",
        "    for branch in branches:\n",
        "        branch_rep = np.zeros(323)\n",
        "        for twid in branch:\n",
        "            if twid in thread_feature_dict.keys():\n",
        "                tweet_rep = thread_feature_dict[twid]\n",
        "                #print (tweet_rep.shape)\n",
        "                branch_rep=np.vstack((branch_rep,tweet_rep))\n",
        "        if branch_rep.shape[0]>=1:\n",
        "            #branch_rep = np.asarray(branch_rep)\n",
        "            thread_features_array.append(branch_rep)\n",
        "     \n",
        "    return thread_features_array\n",
        "\n",
        "def main():\n",
        "    \n",
        "    path = 'Saved_dataset_train_all'\n",
        "    folds = {}\n",
        "    with open('dataset_train_all.json','r') as r:\n",
        "        folds=json.load(r)\n",
        "\n",
        "    \n",
        "    for fold in folds.keys():\n",
        "        \n",
        "        feature_fold = []\n",
        "        labels = []\n",
        "        ids = []\n",
        "        for conversation in folds[fold]:\n",
        "            \n",
        "            thread_feature_dict = extract_thread_features_incl_response(conversation)\n",
        "            thread_features_array = convert_to_array_of_branches(thread_feature_dict, conversation)\n",
        "            feature_fold.extend(thread_features_array)\n",
        "            for i in range(len(thread_features_array)):\n",
        "                labels.append(label_veracity[conversation['veracity']])\n",
        "                ids.append(conversation['id'])\n",
        "        \n",
        "        if feature_fold!=[]:\n",
        "\n",
        "            feature_fold = pad_sequences(feature_fold, maxlen=None,dtype='float32',padding='post',truncating='post', value=0.)\n",
        "            labels = np.asarray(labels)\n",
        "            path_fold = os.path.join(path, fold)\n",
        "            if not os.path.exists(path_fold):\n",
        "                os.makedirs(path_fold)\n",
        "            np.save(os.path.join(path_fold, 'train_array'), feature_fold)\n",
        "            np.save(os.path.join(path_fold, 'ids'), ids)\n",
        "            np.save(os.path.join(path_fold, 'labels'), labels)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1itRP6JwUuh8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "veracity_labels={0:\"true\",1:\"false\",2:\"unverified\"}\n",
        "sdqc_labels={0:\"support\",1:\"deny\",2:\"query\",3:\"comment\"}\n",
        "def save_output(idsa, predictionsa, idsb, predictionsb, confidenceb):\n",
        "    \n",
        "    subtaskaenglish = {}\n",
        "    subtaskbenglish = {}\n",
        "    \n",
        "    for i in range(len(idsa)):\n",
        "      subtaskaenglish[idsa[i]]=sdqc_labels[predictionsa[i]]    \n",
        "\n",
        "    for i in range(len(idsb)):\n",
        "      subtaskbenglish[idsb[i]]=veracity_labels[predictionsb[i]]\n",
        "    answer = {}\n",
        "    answer['subtaskaenglish'] = subtaskaenglish\n",
        "    answer['subtaskbenglish'] = subtaskbenglish\n",
        "    \n",
        "    answer['subtaskadanish'] = {}\n",
        "    answer['subtaskbdanish'] = {}\n",
        "    \n",
        "    answer['subtaskarussian'] = {}\n",
        "    answer['subtaskbrussian'] = {}\n",
        "\n",
        "    #print (answer)\n",
        "    #print (answer[\"subtaskbenglish\"])\n",
        "    with open(\"answer_veracity_1002.json\", 'w') as f:\n",
        "        json.dump(answer, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i6DfIXvRRo9J",
        "colab_type": "code",
        "outputId": "cc922a79-f459-4746-9f61-1eb0e4a578d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6167
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential,model_from_json\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.layers import TimeDistributed, Masking\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from branch2treelabels import branch2treelabels\n",
        "#from Save_Output import save_output,convertsave_competitionformat\n",
        "\n",
        "\n",
        "def LSTM_model_veracity(x_train, y_train, x_test):\n",
        "    model = Sequential()\n",
        "    num_features = x_train.shape[2]\n",
        "    model.add(Masking(mask_value=0., input_shape=(None, num_features)))\n",
        "    model.add(LSTM(300, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\n",
        "    model.add(LSTM(300, dropout=0.2, recurrent_dropout=0.2,return_sequences=False))\n",
        "    model.add(Dense(200, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='softmax',activity_regularizer=regularizers.l2(3e-4)))\n",
        "    adam = optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999,epsilon=1e-08, decay=0.0)\n",
        "    model.compile(optimizer=adam, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train,epochs=100, shuffle=True, class_weight=None)\n",
        "    model_json = model.to_json()\n",
        "    with open(\"model_1002.json\", \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "    model.save_weights(\"model_1002.h5\")\n",
        "    print(\"Saved model to disk\")\n",
        "    \n",
        "    json_file = open('model_1002.json', 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    model = model_from_json(loaded_model_json)\n",
        "    model.load_weights(\"model_1002.h5\")\n",
        "    print(\"Created LSTM Model\")\n",
        "    pred_probabilities = model.predict(x_test)\n",
        "    confidence = np.max(pred_probabilities, axis=1)\n",
        "    Y_pred = model.predict_classes(x_test)\n",
        "    return Y_pred, confidence\n",
        "\n",
        "def main():\n",
        "    x_train = np.load(os.path.join('preprocessing/Saved_dataset_train_all','train/train_array.npy'))\n",
        "    y_train = np.load(os.path.join('preprocessing/Saved_dataset_train_all', 'train/labels.npy'))\n",
        "    y_train = to_categorical(y_train, num_classes=None)\n",
        "    x_test = np.load(os.path.join('preprocessing/Saved_dataset_train_all', 'train/train_array.npy'))\n",
        "    y_test = np.load(os.path.join('preprocessing/Saved_dataset_train_all', 'train/labels.npy'))\n",
        "    ids_test = np.load(os.path.join('preprocessing/Saved_dataset_train_all','train/ids.npy'))\n",
        "    y_pred, confidence = LSTM_model_veracity(x_train, y_train, x_test)\n",
        "    trees, tree_prediction, tree_label,veracity_confidence = branch2treelabels(ids_test, y_test,y_pred,confidence)\n",
        "    \"\"\"\n",
        "    for i in range(len(veracity_confidence)):\n",
        "        if tree_prediction[i]==2:\n",
        "            #veracity_confidence[i]=0\n",
        "            tree_prediction[i]=1\n",
        "    \"\"\"\n",
        "    dummy=[]\n",
        "    save_output(dummy,dummy,trees,tree_prediction,veracity_confidence)\n",
        "    print (\"Charliehedbo event is the Test Data\")\n",
        "    print (trees)\n",
        "    print (\"Predictions\",(tree_prediction))\n",
        "    print (\"Labels\",(tree_label))\n",
        "    print (\"Confidence\",(veracity_confidence))\n",
        "    print (\"Accuracy :\",(accuracy_score(tree_label,tree_prediction)))\n",
        "    from sklearn.metrics import classification_report\n",
        "    print(classification_report(tree_label,tree_prediction,target_names=[\"true\",\"false\",\"unverified\"]))\n",
        "\n",
        "    \n",
        "    import scikitplot as skplt\n",
        "    import matplotlib.pyplot as plt\n",
        "    skplt.metrics.plot_confusion_matrix(tree_label,tree_prediction)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/100\n",
            "4013/4013 [==============================] - 53s 13ms/step - loss: 1.0864 - acc: 0.4067\n",
            "Epoch 2/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 1.0677 - acc: 0.4448\n",
            "Epoch 3/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 1.0446 - acc: 0.4941\n",
            "Epoch 4/100\n",
            "4013/4013 [==============================] - 52s 13ms/step - loss: 1.0264 - acc: 0.4902\n",
            "Epoch 5/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 1.0021 - acc: 0.4966\n",
            "Epoch 6/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.9852 - acc: 0.5088\n",
            "Epoch 7/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.9732 - acc: 0.5213\n",
            "Epoch 8/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.9636 - acc: 0.5235\n",
            "Epoch 9/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.9567 - acc: 0.5358\n",
            "Epoch 10/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.9366 - acc: 0.5420\n",
            "Epoch 11/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.9185 - acc: 0.5472\n",
            "Epoch 12/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.9023 - acc: 0.5736\n",
            "Epoch 13/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.8857 - acc: 0.5794\n",
            "Epoch 14/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.8703 - acc: 0.5906\n",
            "Epoch 15/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.8401 - acc: 0.6113\n",
            "Epoch 16/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.8211 - acc: 0.6290\n",
            "Epoch 17/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.7971 - acc: 0.6524\n",
            "Epoch 18/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.7723 - acc: 0.6686\n",
            "Epoch 19/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.7447 - acc: 0.6815\n",
            "Epoch 20/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.7432 - acc: 0.6790\n",
            "Epoch 21/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.7144 - acc: 0.7022\n",
            "Epoch 22/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.6969 - acc: 0.7172\n",
            "Epoch 23/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.6886 - acc: 0.7102\n",
            "Epoch 24/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.6754 - acc: 0.7269\n",
            "Epoch 25/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.6633 - acc: 0.7336\n",
            "Epoch 26/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.6513 - acc: 0.7406\n",
            "Epoch 27/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.6381 - acc: 0.7403\n",
            "Epoch 28/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.6265 - acc: 0.7418\n",
            "Epoch 29/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.6153 - acc: 0.7536\n",
            "Epoch 30/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.6205 - acc: 0.7478\n",
            "Epoch 31/100\n",
            "4013/4013 [==============================] - 50s 12ms/step - loss: 0.5957 - acc: 0.7593\n",
            "Epoch 32/100\n",
            "4013/4013 [==============================] - 50s 12ms/step - loss: 0.6054 - acc: 0.7618\n",
            "Epoch 33/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.5777 - acc: 0.7670\n",
            "Epoch 34/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.5709 - acc: 0.7717\n",
            "Epoch 35/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.5561 - acc: 0.7752\n",
            "Epoch 36/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.5564 - acc: 0.7775\n",
            "Epoch 37/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.5600 - acc: 0.7740\n",
            "Epoch 38/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.5429 - acc: 0.7785\n",
            "Epoch 39/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.5379 - acc: 0.7862\n",
            "Epoch 40/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.5284 - acc: 0.7869\n",
            "Epoch 41/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.5191 - acc: 0.7924\n",
            "Epoch 42/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.5258 - acc: 0.7859\n",
            "Epoch 43/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.5174 - acc: 0.7959\n",
            "Epoch 44/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.5132 - acc: 0.7952\n",
            "Epoch 45/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.5153 - acc: 0.7947\n",
            "Epoch 46/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4956 - acc: 0.8046\n",
            "Epoch 47/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4895 - acc: 0.8069\n",
            "Epoch 48/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4904 - acc: 0.8109\n",
            "Epoch 49/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.4863 - acc: 0.8044\n",
            "Epoch 50/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4809 - acc: 0.8121\n",
            "Epoch 51/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.4792 - acc: 0.8109\n",
            "Epoch 52/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4709 - acc: 0.8173\n",
            "Epoch 53/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4720 - acc: 0.8121\n",
            "Epoch 54/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4693 - acc: 0.8111\n",
            "Epoch 55/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.4561 - acc: 0.8216\n",
            "Epoch 56/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4532 - acc: 0.8268\n",
            "Epoch 57/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4530 - acc: 0.8226\n",
            "Epoch 58/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4355 - acc: 0.8253\n",
            "Epoch 59/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4318 - acc: 0.8203\n",
            "Epoch 60/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4286 - acc: 0.8303\n",
            "Epoch 61/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.4250 - acc: 0.8330\n",
            "Epoch 62/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4265 - acc: 0.8298\n",
            "Epoch 63/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4293 - acc: 0.8268\n",
            "Epoch 64/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4263 - acc: 0.8373\n",
            "Epoch 65/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3946 - acc: 0.8453\n",
            "Epoch 66/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4128 - acc: 0.8388\n",
            "Epoch 67/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3964 - acc: 0.8423\n",
            "Epoch 68/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.4015 - acc: 0.8430\n",
            "Epoch 69/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3866 - acc: 0.8487\n",
            "Epoch 70/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3976 - acc: 0.8460\n",
            "Epoch 71/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3954 - acc: 0.8420\n",
            "Epoch 72/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3660 - acc: 0.8510\n",
            "Epoch 73/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3854 - acc: 0.8495\n",
            "Epoch 74/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3816 - acc: 0.8462\n",
            "Epoch 75/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3864 - acc: 0.8425\n",
            "Epoch 76/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3657 - acc: 0.8547\n",
            "Epoch 77/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3758 - acc: 0.8547\n",
            "Epoch 78/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3556 - acc: 0.8547\n",
            "Epoch 79/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3662 - acc: 0.8490\n",
            "Epoch 80/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3635 - acc: 0.8550\n",
            "Epoch 81/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3581 - acc: 0.8562\n",
            "Epoch 82/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3494 - acc: 0.8580\n",
            "Epoch 83/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3518 - acc: 0.8587\n",
            "Epoch 84/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3297 - acc: 0.8697\n",
            "Epoch 85/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3321 - acc: 0.8682\n",
            "Epoch 86/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3569 - acc: 0.8570\n",
            "Epoch 87/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3365 - acc: 0.8694\n",
            "Epoch 88/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3290 - acc: 0.8712\n",
            "Epoch 89/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3399 - acc: 0.8654\n",
            "Epoch 90/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3322 - acc: 0.8654\n",
            "Epoch 91/100\n",
            "4013/4013 [==============================] - 50s 12ms/step - loss: 0.3244 - acc: 0.8689\n",
            "Epoch 92/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3371 - acc: 0.8699\n",
            "Epoch 93/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3200 - acc: 0.8774\n",
            "Epoch 94/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3130 - acc: 0.8769\n",
            "Epoch 95/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3106 - acc: 0.8752\n",
            "Epoch 96/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3064 - acc: 0.8789\n",
            "Epoch 97/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3200 - acc: 0.8704\n",
            "Epoch 98/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3061 - acc: 0.8791\n",
            "Epoch 99/100\n",
            "4013/4013 [==============================] - 50s 13ms/step - loss: 0.3037 - acc: 0.8816\n",
            "Epoch 100/100\n",
            "4013/4013 [==============================] - 51s 13ms/step - loss: 0.3083 - acc: 0.8811\n",
            "Saved model to disk\n",
            "Created LSTM Model\n",
            "Charliehedbo event is the Test Data\n",
            "['498280126254428160' '498293668655423488' '498430783699554305'\n",
            " '498486826269548545' '499366666300846081' '499368931367608320'\n",
            " '499456140044824576' '499530130487017472' '499612545909415938'\n",
            " '500258409988763649' '500270780832174080' '500278045597368320'\n",
            " '500279160795721728' '500279189405433858' '500280249629036544'\n",
            " '500280422295937024' '500280838710247424' '500281094239817728'\n",
            " '500281131057811456' '500284699546517505' '500286058664579072'\n",
            " '500288349924782080' '500290456845299714' '500294803402137600'\n",
            " '500295393301647360' '500298588992593920' '500298752469770240'\n",
            " '500303431928922113' '500307001629745152' '500308076004929537'\n",
            " '500319675797209088' '500319801344929795' '500327106824245249'\n",
            " '500327120770301952' '500332933098385408' '500341884678836224'\n",
            " '500347114975944705' '500354773133299713' '500363126294863876'\n",
            " '500363740311982081' '500371149713178625' '500377145349521411'\n",
            " '500377906305327104' '500378223977721856' '500378522788315137'\n",
            " '500381163866062848' '500389488217309184' '500391222075076610'\n",
            " '500394061887709184' '500413818368184321' '521346721226711040'\n",
            " '521360486387175424' '524922729485848576' '524923293711998976'\n",
            " '524923462398513152' '524923676484177920' '524924619812511746'\n",
            " '524925050739490816' '524925215235911680' '524925730053181440'\n",
            " '524925987239120897' '524926235030589440' '524926472432410625'\n",
            " '524927281048080385' '524929497205055488' '524931324763992064'\n",
            " '524932056560963584' '524932935137628160' '524935485370929152'\n",
            " '524936793633083394' '524936872666353664' '524937542131793920'\n",
            " '524940659778920448' '524941132237910016' '524941720249978880'\n",
            " '524942470472548352' '524943490887991296' '524944399890124801'\n",
            " '524947416869388288' '524947674164760577' '524947867975561216'\n",
            " '524948206023880704' '524948866773184512' '524949443607412737'\n",
            " '524952883343925249' '524956129017995264' '524959809402331137'\n",
            " '524961721744900097' '524962142563610625' '524964948683005952'\n",
            " '524965775036387329' '524966904885428226' '524969201102901248'\n",
            " '524970851675176960' '524972443308683264' '524975705206304769'\n",
            " '524980744658382848' '524981436252950528' '524983581983375360'\n",
            " '524990163446140928' '524991576163250176' '524993533212897281'\n",
            " '524995771587108864' '525003468659228672' '525019752507658240'\n",
            " '525023025792835585' '525025279803424768' '525025463648137216'\n",
            " '525028734991343617' '525032872647065600' '525049639016615937'\n",
            " '525056576038518785' '525058976376193024' '525060425184858112'\n",
            " '525068915068923904' '529540733020405760' '529653029747064832'\n",
            " '529654186791944192' '529660296080916480' '529687410611728384'\n",
            " '529689679411810304' '529695367680761856' '529695483661664257'\n",
            " '529713467184676864' '529716453792956416' '529720273285566464'\n",
            " '529739968470867968' '544268732046913536' '544269749405097984'\n",
            " '544271069146656768' '544271284796784640' '544271362022338560'\n",
            " '544272537341812736' '544274544174071809' '544274934835707905'\n",
            " '544277117039837184' '544277860555710464' '544278335455776769'\n",
            " '544278985249550337' '544282005941530624' '544282227035869184'\n",
            " '544283772569788416' '544287209730236416' '544288681021145090'\n",
            " '544289311504355328' '544289409294553088' '544289941996326912'\n",
            " '544290258951892992' '544291804057960448' '544291965513134080'\n",
            " '544292129972170752' '544292670336925696' '544293753130082305'\n",
            " '544294893146091520' '544297696308518912' '544301453717041152'\n",
            " '544305540286148609' '544305745416581120' '544306402731507712'\n",
            " '544306719686656000' '544309275141885952' '544310853613281281'\n",
            " '544314234541469696' '544315472075042818' '544319274072817664'\n",
            " '544319832486064128' '544324444773433348' '544328894812549121'\n",
            " '544329935943237632' '544333764814323713' '544350567183556608'\n",
            " '544350712365207552' '544352727971954690' '544358533819420672'\n",
            " '544358564484378624' '544367462012432384' '544374511194632192'\n",
            " '544380742076088320' '544381485591982083' '544382892378714113'\n",
            " '544391176137089024' '544391533240516608' '544399927045283840'\n",
            " '544462330105712640' '544476808566276097' '544491151118860289'\n",
            " '544504183341064192' '544510450101415936' '544511199702822913'\n",
            " '544512108885725184' '544512664769396736' '544512676643500033'\n",
            " '544512910538838016' '544513524438155264' '544514564407427072'\n",
            " '544514570367168512' '544515538383564801' '544517264054423552'\n",
            " '544518335019229184' '544520042810200064' '544520273718812672'\n",
            " '552783667052167168' '552785375161499649' '552788945017516032'\n",
            " '552791196247269378' '552791578893619200' '552792544132997121'\n",
            " '552792802309181440' '552792913910833152' '552793679082311680'\n",
            " '552802654641225728' '552805488631758849' '552806309540528128'\n",
            " '552806757672964097' '552810448324943872' '552811386259386370'\n",
            " '552814494381256704' '552816020403269632' '552821069036670976'\n",
            " '552832817089236992' '552833028201144320' '552834961762709505'\n",
            " '552848620375261184' '552978099357237248' '552978184413921281'\n",
            " '552982613288157184' '552984502063337472' '552996335319007233'\n",
            " '553107921081749504' '553152395371630592' '553160652567498752'\n",
            " '553164985460068352' '553184482241814530' '553197863971610624'\n",
            " '553212962044149761' '553221600955621376' '553461741917863936'\n",
            " '553467311261503488' '553470492565602305' '553474188259102720'\n",
            " '553476490315431937' '553476880339599360' '553478289474740224'\n",
            " '553480082996879360' '553486439129038848' '553489393202499584'\n",
            " '553501357156876290' '553503184174710784' '553505242554175489'\n",
            " '553506608203169792' '553508098825261056' '553512735192141826'\n",
            " '553518472798683136' '553531413459660800' '553534838880608256'\n",
            " '553535829017370625' '553538058440941568' '553543369604210689'\n",
            " '553544252563935234' '553544694765215745' '553548567420628992'\n",
            " '553549686129561600' '553550301886955520' '553553288625672192'\n",
            " '553558982476828674' '553561170637238272' '553566026030272512'\n",
            " '553575232867672064' '553576010898497536' '553579224402235393'\n",
            " '553586860334010368' '553586897168392192' '553587013409325058'\n",
            " '553587303172833280' '553587672137334785' '553588178687655936'\n",
            " '553589051044151296' '553590459688570880' '553590835850514433'\n",
            " '576276947648405505' '576319832800555008' '576323086888361984'\n",
            " '576513463738109954' '576755174531862529' '576796432730071040'\n",
            " '576812998418939904' '576829262927413248' '577258317942149120'\n",
            " '580319078155468800' '580319184652890113' '580320684305416192'\n",
            " '580321156508577792' '580322453928431617' '580323060533764097'\n",
            " '580324027715063808' '580325090367315968' '580326222107951104'\n",
            " '580331561398108160' '580332109782466561' '580333763512705025'\n",
            " '580333909008871424' '580339547269144576' '580339825649291264'\n",
            " '580340476949086208' '580348081100734464' '580352273001410560'\n",
            " '580360165540642816' '580371845997682688' '580882341880446977'\n",
            " '581047170637381632' '581063377226637312' '581153923987206146'\n",
            " '581290271997968384' '581293286268129280' '581359544682614784'\n",
            " '581386094337474560' '581473088249958400' '758159624122097664'\n",
            " '763098277986209792' '764927075522260992' '767725956706414592'\n",
            " '768859780240773121' '769988636754505729' '774991078265094144'\n",
            " '775057555865206784']\n",
            "Predictions [2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 2, 1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 1, 1, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 2, 0, 1, 0, 2, 0, 1, 1, 0, 1, 2, 1, 1, 2, 2, 1, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 0, 2, 2, 0, 1, 1, 1, 1, 0, 0, 2, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Labels [2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 1, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0, 0, 2, 1, 1, 1, 2, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 2, 1, 1, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 2, 2, 0, 1, 0, 2, 1, 1, 1, 0, 1, 0, 1, 0, 2, 2, 1, 2, 0, 2, 2, 2, 2, 0, 0, 1, 1, 1, 1, 2, 2, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 0, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 1, 2, 1, 1, 0, 0, 2, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Confidence [0.9835343, 0.9997793, 0.7617822, 0.9857151, 0.9652847, 0.99935097, 0.93773365, 0.986624, 0.99981254, 0.73750025, 0.9882762, 0.9761114, 0.99999976, 0.9751913, 0.9996668, 0.99375135, 0.999848, 0.99195194, 0.99805754, 0.99959797, 0.9999293, 0.9907699, 0.99740124, 0.9948866, 0.9365662, 0.97988963, 0.9999999, 0.99938, 0.99999607, 0.99246454, 0.98212194, 0.9477576, 0.88158405, 0.9868478, 0.99175054, 0.9989666, 0.99997234, 0.98624134, 0.7844997, 0.9945175, 0.9998903, 0.99933773, 0.9991191, 0.715126, 0.9984927, 0.9884459, 0.999938, 0.9999956, 0.998626, 0.988732, 0.99845994, 0.98605007, 0.9973252, 0.9421011, 0.9687449, 0.9287229, 0.50899124, 0.9398136, 0.9413449, 0.723849, 0.88283694, 0.96041465, 0.5660137, 0.9997098, 0.8159698, 0.9635007, 0.8736119, 0.9999753, 0.97693515, 0.67483145, 0.9694185, 0.6974493, 0.57896215, 0.75679576, 0.9963012, 0.9377711, 0.93391097, 0.90238297, 0.92693585, 0.9995634, 0.66256684, 0.9797029, 0.67060417, 0.5806871, 0.9765842, 0.9938386, 0.97833294, 0.99445, 0.9984383, 0.99641454, 0.7772778, 0.99979275, 0.94569254, 0.45231184, 0.930287, 0.9474221, 0.9019309, 0.77616113, 0.92136854, 0.9875652, 0.4457236, 0.7387703, 0.686044, 0.9598017, 0.98361105, 0.91624653, 0.96132195, 0.9900177, 0.8210854, 0.84431326, 0.9039578, 0.9928411, 0.9044047, 0.98177344, 0.9442781, 0.9455439, 0.64771104, 0.95900196, 0.9673391, 0.99781597, 0.49618208, 0.99153167, 0.99750996, 0.9678276, 0.9911651, 0.99999976, 0.99902844, 0.9663123, 0.8998398, 0.7153915, 0.78094685, 0.99970883, 0.6742761, 0.99444795, 0.9899305, 0.99621934, 0.9316806, 0.9849028, 0.9977076, 0.7217165, 0.7526921, 0.99203885, 0.60397375, 0.53626835, 0.7125033, 0.6704852, 0.94969904, 0.5054149, 0.9804581, 0.9950199, 0.9951382, 0.6224298, 0.9426381, 0.8668648, 0.9604186, 0.77183086, 0.6550014, 0.42646867, 0.8742627, 0.996042, 0.9835497, 0.9262921, 0.9559722, 0.7111684, 0.9824437, 0.8375195, 0.88669544, 0.9999738, 0.99975616, 0.8803771, 0.8216979, 0.6430094, 0.6285023, 0.9257759, 0.48801857, 0.90692735, 0.60658056, 0.842781, 0.9847995, 0.75213873, 0.94987327, 0.5648592, 0.64640826, 0.99990773, 0.9639217, 0.72644913, 0.9078218, 0.46430317, 0.9952743, 0.9529775, 0.5603072, 0.943694, 0.5414871, 0.55703706, 0.4974239, 0.9775354, 0.93472433, 0.98997635, 0.8478331, 0.9896688, 0.9796438, 0.5033113, 0.9390602, 0.97811, 0.968257, 0.81901574, 0.93040603, 0.97337276, 0.4650196, 0.55762845, 0.78446066, 0.9848846, 0.9999863, 0.7979805, 0.9781528, 0.9216854, 0.5672897, 0.9284139, 0.54631984, 0.5038877, 0.992343, 0.7448459, 0.9862705, 0.6993301, 0.7957878, 0.90609306, 0.5015773, 0.48901007, 0.5382222, 0.9706442, 0.5118354, 0.831043, 0.98723024, 0.8346078, 0.61023, 0.803369, 0.80946594, 0.35131705, 0.59565043, 0.64317465, 0.50667155, 0.79198676, 0.38914928, 0.48529077, 0.41437027, 0.54445386, 0.99994206, 0.99585694, 0.95616394, 0.8388679, 0.6324547, 0.7278557, 0.92190254, 0.63736403, 0.99952877, 0.6190761, 0.60987765, 0.7438442, 0.6330959, 0.7551234, 0.9818942, 0.98730564, 0.56295043, 0.99999297, 0.9999851, 0.9977952, 0.9622133, 0.8022054, 0.6211669, 0.9289504, 0.9999306, 0.78737336, 0.98881537, 0.9692834, 0.9782748, 0.62989074, 0.9331414, 0.7620842, 0.94369674, 0.90006244, 0.6435271, 0.83934814, 0.9445448, 0.7443738, 0.8015955, 0.68706673, 0.8440909, 0.9543341, 0.7983083, 0.9026921, 0.88584626, 0.8395419, 0.87405723, 0.6977156, 0.50760555, 0.97203076, 0.96703243, 0.8636308, 0.9689801, 0.9978771, 0.5300112, 0.9041921, 0.9672423, 0.9926604, 0.68292093, 0.999972, 0.98090404, 0.9170451, 0.63057536, 0.71869504, 0.5163665, 0.92739403, 0.9999114, 0.9998086, 0.9881334, 0.7599072, 0.97205263, 0.99999595, 0.99803036, 0.99999964, 1.0, 0.99962604, 0.9999999, 0.99999964, 0.99963486]\n",
            "Accuracy : 0.8892307692307693\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        true       0.85      0.94      0.89       145\n",
            "       false       0.92      0.81      0.86        74\n",
            "  unverified       0.94      0.87      0.90       106\n",
            "\n",
            "   micro avg       0.89      0.89      0.89       325\n",
            "   macro avg       0.90      0.87      0.89       325\n",
            "weighted avg       0.89      0.89      0.89       325\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-455494f7275a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-455494f7275a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mscikitplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mskplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mskplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtree_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scikitplot'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}